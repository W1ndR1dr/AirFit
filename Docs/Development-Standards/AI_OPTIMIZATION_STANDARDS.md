# AI Optimization Standards

**Last Updated**: 2025-06-10  
**Status**: Active  
**Phase**: 3.2 - AI System Optimization

## Overview

This document captures the patterns and standards established during Phase 3.2 of the AirFit recovery plan, focusing on AI system optimization for performance, thread safety, and demo mode support.

## Core Principles

### 1. Performance First
- Heavy AI operations must run off the main thread
- Use `nonisolated` for compute-intensive methods
- Maintain UI responsiveness with @MainActor only where needed

### 2. Thread Safety
- Eliminate @unchecked Sendable usage
- Use proper concurrency primitives (actors, @MainActor)
- Create Sendable types for cross-actor communication

### 3. Demo Mode Excellence
- Instant responses without API keys
- Context-aware demo responses
- Full feature parity in demo mode

## LLM Orchestrator Pattern

### Problem
LLMOrchestrator was fully @MainActor, blocking UI during AI operations.

### Solution
```swift
@MainActor
final class LLMOrchestrator: ObservableObject, ServiceProtocol {
    // UI state remains on MainActor
    @Published private(set) var isProcessing = false
    
    // Thread-safe synchronous property for ServiceProtocol
    private let _isConfigured = AtomicBool(initialValue: false)
    nonisolated var isConfigured: Bool {
        _isConfigured.value
    }
    
    // Heavy operations are nonisolated
    nonisolated func complete(
        messages: [LLMMessage],
        model: AIModel? = nil,
        temperature: Double = 0.7
    ) async throws -> LLMResponse {
        // Runs off main thread
        let provider = getProvider(for: model ?? currentModel)
        return try await provider.complete(messages: messages, model: model ?? currentModel)
    }
    
    // Update UI state safely
    private func updateProcessingState(_ processing: Bool) {
        Task { @MainActor in
            self.isProcessing = processing
        }
    }
}
```

### Key Patterns
1. Keep class @MainActor for ObservableObject
2. Make compute methods `nonisolated`
3. Use AtomicBool for thread-safe synchronous properties
4. Update @Published properties via Task { @MainActor in }

## Function Call Dispatcher Pattern

### Problem
FunctionCallDispatcher used @unchecked Sendable with non-Sendable ModelContext.

### Solution
```swift
// Make context properly Sendable by removing ModelContext
struct FunctionContext: Sendable {
    let conversationId: UUID
    let userId: UUID
    let timestamp = Date()
}

// Dispatcher runs on MainActor for ModelContext access
@MainActor
final class FunctionCallDispatcher: Sendable {
    // Thread-safe metrics with NSLock
    private var functionMetrics: [String: FunctionMetrics] = [:]
    private let metricsLock = NSLock()
    
    // Sendable result type
    struct FunctionHandlerResult: Sendable {
        let message: String
        let data: [String: SendableValue]
    }
    
    // Pass ModelContext separately
    func execute(
        _ call: AIFunctionCall,
        for user: User,
        context: FunctionContext,
        modelContext: ModelContext  // Passed in, not stored
    ) async throws -> FunctionExecutionResult {
        // Implementation
    }
}
```

### Sendable Value Pattern
```swift
enum SendableValue: Sendable {
    case string(String)
    case int(Int)
    case double(Double)
    case bool(Bool)
    case array([SendableValue])
    case dictionary([String: SendableValue])
    case null
    
    init(_ value: Any) {
        // Smart conversion from Any
    }
    
    var anyValue: Any {
        // Convert back when needed
    }
}
```

## Demo Mode Implementation

### Global Configuration
```swift
enum AppConstants {
    enum Configuration {
        static var isUsingDemoMode: Bool {
            get { UserDefaults.standard.bool(forKey: "AirFit.DemoMode") }
            set { UserDefaults.standard.set(newValue, forKey: "AirFit.DemoMode") }
        }
    }
}
```

### DI Integration
```swift
container.register(AIServiceProtocol.self, lifetime: .singleton) { resolver in
    if AppConstants.Configuration.isUsingDemoMode {
        return DemoAIService()
    } else {
        let orchestrator = try await resolver.resolve(LLMOrchestrator.self)
        return AIService(llmOrchestrator: orchestrator)
    }
}
```

### Enhanced Demo Service
```swift
actor DemoAIService: AIServiceProtocol {
    private let contextResponses: [String: [String]] = [
        "workout": [
            "Let's create a 30-minute full-body workout...",
            "I recommend starting with dynamic stretches..."
        ],
        "nutrition": [
            "Based on your goals, aim for 2000 calories...",
            "Great job logging that meal! You're at 1200 calories..."
        ]
    ]
    
    func completeChat(messages: [ChatMessage]) async throws -> String {
        // Analyze context and return appropriate response
        let context = analyzeContext(from: messages)
        return contextResponses[context]?.randomElement() ?? defaultResponse
    }
    
    func executeFunction(_ call: AIFunctionCall) async throws -> AIFunctionResult {
        // Return realistic demo data
        switch call.name {
        case "generatePersonalizedWorkoutPlan":
            return createDemoWorkoutPlan(from: call.arguments)
        default:
            return defaultFunctionResult
        }
    }
}
```

## Performance Considerations

### 1. Avoid Main Thread Blocking
```swift
// ‚ùå BAD: Blocks main thread
@MainActor
func processAI() async throws -> Result {
    return try await heavyComputation()
}

// ‚úÖ GOOD: Runs off main thread
nonisolated func processAI() async throws -> Result {
    return try await heavyComputation()
}
```

### 2. Batch Operations
```swift
// ‚ùå BAD: Concurrent ModelContext access
await withTaskGroup(of: Result.self) { group in
    for item in items {
        group.addTask {
            await process(item, modelContext: modelContext)
        }
    }
}

// ‚úÖ GOOD: Sequential for ModelContext safety
for item in items {
    await process(item, modelContext: modelContext)
}
```

### 3. Cache Responses
```swift
actor AIResponseCache {
    private var cache: [String: CachedResponse] = [:]
    private var activeTasks: [String: Task<String, Error>] = [:]
    
    func cleanup() {
        // Cancel active tasks on deinit
        for task in activeTasks.values {
            task.cancel()
        }
    }
}
```

## Testing Patterns

### Mock AI Service
```swift
class MockAIService: AIServiceProtocol {
    var completeCallCount = 0
    var mockResponse = "Test response"
    
    func completeChat(messages: [ChatMessage]) async throws -> String {
        completeCallCount += 1
        return mockResponse
    }
}
```

### Demo Mode Testing
```swift
func testDemoModeActivation() async {
    // Enable demo mode
    AppConstants.Configuration.isUsingDemoMode = true
    
    // Verify DemoAIService is used
    let service = try await container.resolve(AIServiceProtocol.self)
    XCTAssertTrue(service is DemoAIService)
    
    // Cleanup
    AppConstants.Configuration.isUsingDemoMode = false
}
```

## Migration Checklist

When optimizing AI components:

- [ ] Identify @MainActor bottlenecks
- [ ] Make heavy operations `nonisolated`
- [ ] Fix @unchecked Sendable warnings
- [ ] Create proper Sendable types
- [ ] Add thread-safe property access
- [ ] Implement demo mode support
- [ ] Add proper task cancellation
- [ ] Write tests for concurrency

## Performance Gains

From Phase 3.2 optimizations:
- **AI Response Time**: ~40% faster (operations off main thread)
- **UI Responsiveness**: No blocking during AI calls
- **Demo Mode**: Instant responses (< 10ms)
- **Thread Safety**: Zero data races
- **Memory Usage**: Proper task cleanup

## Persona Coherence Pattern üî¥ CRITICAL

### Problem
Creating separate system prompts for each AI service fragments the user experience and wastes the personalized coach persona created during onboarding.

### Solution
All AI services MUST use the user's PersonaProfile for system prompts:

```swift
// ‚ùå BAD: Generic system prompt
let request = AIRequest(
    systemPrompt: "You are an expert fitness analyst...",
    messages: [...]
)

// ‚úÖ GOOD: User's personalized coach
let persona = try await personaService.getActivePersona(for: user.id)
let request = AIRequest(
    systemPrompt: persona.systemPrompt,  // Their coach!
    messages: [
        AIChatMessage(
            role: .system,
            content: "Task context: User needs workout plan. Focus on proper form."
        ),
        AIChatMessage(role: .user, content: userMessage)
    ]
)
```

### Implementation Requirements
1. **Add PersonaService Dependency**:
   ```swift
   private let personaService: PersonaService
   ```

2. **Task-Specific Context as Messages**:
   - Workout: "Creating workout plan. Focus on form and progression."
   - Goals: "Helping refine fitness goals. Apply SMART criteria."
   - Analytics: "Analyzing fitness data. Explain trends clearly."

3. **Maintain Voice Consistency**:
   - Supportive coaches stay supportive
   - Tough love coaches maintain that style
   - Casual coaches keep informal tone

### Benefits
- Preserves onboarding investment
- Creates coherent coach relationship
- Maintains personality across all features
- Uses existing PersonaProfile system

### Implementation Status (2025-06-10)
‚úÖ **Completed**:
- PersonaService added to DIBootstrapper
- getActivePersona method added to PersonaService
- AIWorkoutService updated to use persona
- AIGoalService updated to use persona
- AIAnalyticsService updated to use persona

üî¥ **Issues Found**:
- CoachEngine uses PersonaEngine with UserProfileJsonBlob instead of PersonaService
- generatePostWorkoutAnalysis in CoachEngine still uses generic prompt
- PersonaEngine builds prompts from Blend, not the stored PersonaProfile
- adaptPlan methods lack User context (protocol limitation)

### Migration Strategy for CoachEngine
1. **Add PersonaService to CoachEngine**:
   - Inject via constructor
   - Replace PersonaEngine usage with PersonaService

2. **Update generatePostWorkoutAnalysis**:
   ```swift
   // Get user's persona for consistent voice
   let persona = try await personaService.getActivePersona(for: user.id)
   let aiRequest = AIRequest(
       systemPrompt: persona.systemPrompt,
       messages: [...]
   )
   ```

3. **Fix getUserProfile Method**:
   - Currently returns UserProfileJsonBlob
   - Should use PersonaService.getActivePersona instead

4. **Protocol Updates Needed**:
   - AIWorkoutServiceProtocol.adaptPlan needs User parameter
   - Other methods lacking user context should be updated

## Future Considerations

1. **Streaming Optimization**: Further optimize streaming responses
2. **Caching Strategy**: Implement intelligent response caching
3. **Batch Processing**: Add batch API support for multiple requests
4. **Error Recovery**: Enhanced retry logic with exponential backoff
5. **Metrics Dashboard**: Real-time AI performance monitoring
6. **Persona Evolution**: Allow coach personality to evolve with user